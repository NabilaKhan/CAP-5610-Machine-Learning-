{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "from sklearn.metrics import classification_report, make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing Data and Creating Dataframe\n",
    "train_df = pd.read_csv('input/train.csv')\n",
    "test_df = pd.read_csv('input/test.csv')\n",
    "# train_df.info()\n",
    "# print(train_df.describe())\n",
    "# train_df.head(8)\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Checking for columns with null values in Training Data\n",
    "\n",
    "### Checking for null values in features \"Pclass\"\n",
    "# bool_series = pd.isnull(train_df[\"Pclass\"]) \n",
    "# train_df[bool_series]\n",
    "\n",
    "### Checking for null values in features \"Sex\"\n",
    "# bool_series = pd.isnull(train_df[\"Sex\"]) \n",
    "# train_df[bool_series] \n",
    "\n",
    "### Checking for null values in features \"Age\"\n",
    "# bool_series = pd.isnull(train_df[\"Age\"])     #has empty values\n",
    "# train_df[bool_series] \n",
    "\n",
    "### Checking for null values in features \"SibSp\"\n",
    "# bool_series = pd.isnull(train_df[\"SibSp\"]) \n",
    "# train_df[bool_series] \n",
    "\n",
    "### Checking for null values in features \"Parch\"\n",
    "# bool_series = pd.isnull(train_df[\"Parch\"]) \n",
    "# train_df[bool_series] \n",
    "\n",
    "### Checking for null values in features \"Fare\"\n",
    "# bool_series = pd.isnull(train_df[\"Fare\"]) \n",
    "# train_df[bool_series] \n",
    "\n",
    "### Checking for null values in features \"Embarked\"\n",
    "# bool_series = pd.isnull(train_df[\"Embarked\"])     #has empty values\n",
    "# train_df[bool_series]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Checking for columns with null values in Testing Data\n",
    "\n",
    "### Checking for null values in features \"Pclass\"\n",
    "# bool_series = pd.isnull(test_df[\"Pclass\"]) \n",
    "# test_df[bool_series]\n",
    "\n",
    "### Checking for null values in features \"Sex\"\n",
    "# bool_series = pd.isnull(test_df[\"Sex\"]) \n",
    "# test_df[bool_series] \n",
    "\n",
    "### Checking for null values in features \"Age\"\n",
    "# bool_series = pd.isnull(test_df[\"Age\"])     #has empty values\n",
    "# test_df[bool_series] \n",
    "\n",
    "### Checking for null values in features \"SibSp\"\n",
    "# bool_series = pd.isnull(test_df[\"SibSp\"]) \n",
    "# test_df[bool_series] \n",
    "\n",
    "### Checking for null values in features \"Parch\"\n",
    "# bool_series = pd.isnull(test_df[\"Parch\"]) \n",
    "# test_df[bool_series] \n",
    "\n",
    "### Checking for null values in features \"Fare\"\n",
    "# bool_series = pd.isnull(test_df[\"Fare\"])    #has empty values\n",
    "# test_df[bool_series] \n",
    "\n",
    "### Checking for null values in features \"Embarked\"\n",
    "# bool_series = pd.isnull(test_df[\"Embarked\"])    \n",
    "# test_df[bool_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0\n",
      "1      1\n",
      "2      0\n",
      "3      0\n",
      "4      1\n",
      "      ..\n",
      "413    0\n",
      "414    1\n",
      "415    0\n",
      "416    0\n",
      "417    0\n",
      "Name: Sex, Length: 418, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "### Converting Categorial Feature \"Sex\" to Numeric value [male-> 0, female -> 1]\n",
    "gender_map = {'female':1, 'male':0}\n",
    "for dataset in combine:\n",
    "    dataset['Sex'] = dataset['Sex'].map(gender_map).astype('int32')\n",
    "\n",
    "print(dataset['Sex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "### Handling Feature \"Age\"\n",
    "\n",
    "### Filling up missing values in \"Age\" using KNN\n",
    "print(train_df['Age'].isnull().sum())\n",
    "print(test_df['Age'].isnull().sum())\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\n",
    "\n",
    "for dataset in combine:    \n",
    "    Age_val = dataset['Age'].values\n",
    "    Age_val = Age_val.reshape(-1,1)\n",
    "    imputer.fit(Age_val)\n",
    "    Xtrans = imputer.transform(Age_val)\n",
    "    dataset['Age'] = Xtrans\n",
    "    \n",
    "# print(train_df['Age'].isnull().sum())\n",
    "# print(test_df['Age'].isnull().sum())\n",
    "\n",
    "### Converting Continuous Feature \"Age\" to Ordinal Value\n",
    "\n",
    "for dataset in combine:\n",
    "    for index, row in dataset.iterrows():\n",
    "        if(row['Age'] >= 0 and row['Age'] <= 12):\n",
    "            dataset.loc[index, 'Age'] = 0\n",
    "        elif(row['Age'] > 12 and row['Age'] <= 18):\n",
    "            dataset.loc[index, 'Age'] = 1\n",
    "        elif(row['Age'] > 18 and row['Age'] <= 24):\n",
    "            dataset.loc[index, 'Age'] = 2\n",
    "        elif(row['Age'] > 24 and row['Age'] <= 30):\n",
    "            dataset.loc[index, 'Age'] = 3\n",
    "        elif(row['Age'] > 30 and row['Age'] <= 40):\n",
    "            dataset.loc[index, 'Age'] = 4\n",
    "        elif(row['Age'] > 40 and row['Age'] <= 64):\n",
    "            dataset.loc[index, 'Age'] = 5\n",
    "        elif(row['Age'] > 64):\n",
    "            dataset.loc[index, 'Age'] = 6\n",
    "    dataset['Age'] = train_df['Age'].astype(int) \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Handling Feature \"Cabin\"\n",
    "\n",
    "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"X\": 8}\n",
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Cabin'] = dataset['Cabin'].fillna(\"X0\")\n",
    "    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    dataset['Deck'] = dataset['Deck'].map(deck)\n",
    "    dataset['Deck'] = dataset['Deck'].fillna(0)\n",
    "    dataset['Deck'] = dataset['Deck'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Handling feature \"Embarked\"\n",
    "\n",
    "### Filling up missing values in \"Embarked\" using Highest Frquency\n",
    "emb_high_freq_train = train_df.Embarked.dropna().mode()[0]\n",
    "emb_high_freq_train_count = train_df[\"Embarked\"].value_counts().nlargest(n=1).values[0]\n",
    "emb_high_freq_test = test_df.Embarked.dropna().mode()[0]\n",
    "emb_high_freq_test_count = test_df[\"Embarked\"].value_counts().nlargest(n=1).values[0]\n",
    "\n",
    "if emb_high_freq_train_count >= emb_high_freq_test_count:\n",
    "    emb_high_freq = emb_high_freq_train\n",
    "else:\n",
    "    emb_high_freq = emb_high_freq_test\n",
    "    \n",
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna(emb_high_freq)\n",
    "\n",
    "\n",
    "### Converting Categorial Feature \"Embarked\" to Numeric value [S-> 0, C -> 1, Q -> 2] \n",
    "\n",
    "emb_map = {'S': 0, 'C': 1, 'Q': 2}\n",
    "for dataset in combine:\n",
    "    dataset['Embarked'] = dataset['Embarked'].map(emb_map).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Handling feature \"Fare\"\n",
    "\n",
    "### Filling up missing values in \"Embarked\" using Highest Frquency\n",
    "fare_med = test_df.Fare.dropna().median()\n",
    "dataset['Fare'] = dataset['Fare'].fillna(fare_med) \n",
    "\n",
    "### Converting Continuous Feature \"Fare\" to Ordinal Value\n",
    "\n",
    "for dataset in combine:\n",
    "    for index, row in dataset.iterrows():\n",
    "        if(row['Fare'] > -0.001 and row['Fare'] <= 7.91):\n",
    "            dataset.loc[index, 'Fare'] = 0\n",
    "        elif(row['Fare'] > 7.91 and row['Fare'] <= 14.454):\n",
    "            dataset.loc[index, 'Fare'] = 1\n",
    "        elif(row['Fare'] > 14.454 and row['Fare'] <= 31):\n",
    "            dataset.loc[index, 'Fare'] = 2\n",
    "        elif(row['Fare'] > 31 and row['Fare'] <= 512.329):\n",
    "            dataset.loc[index, 'Fare'] = 3\n",
    "    \n",
    "train_df['Fare'] = train_df['Fare'].astype(int)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Handling feature \"SibSp\" and \"Parch\"\n",
    "for dataset in combine:\n",
    "    dataset['Family'] = dataset['SibSp'] + dataset['Parch']\n",
    "    dataset.loc[dataset['Family'] > 0, 'Alone'] = 0\n",
    "    dataset.loc[dataset['Family'] == 0, 'Alone'] = 1\n",
    "    dataset['Alone'] = dataset['Alone'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass  Sex  Age  Fare  Embarked  Deck  Family  Alone\n",
      "0         0       3    0    2     0         0     8       1      0\n",
      "1         1       1    1    4     3         1     3       1      0\n",
      "2         1       3    1    3     1         0     8       0      1\n",
      "3         1       1    1    4     3         0     3       1      0\n",
      "4         0       3    0    4     1         0     8       0      1\n",
      "   Pclass  Sex  Age  Fare  Embarked  Deck  Family  Alone\n",
      "0       3    0    2   0.0         2     8       0      1\n",
      "1       3    1    4   0.0         0     8       1      0\n",
      "2       2    0    3   1.0         2     8       0      1\n",
      "3       3    0    4   1.0         0     8       0      1\n",
      "4       3    1    4   1.0         0     8       2      0\n"
     ]
    }
   ],
   "source": [
    "### Drop Columns \n",
    "train_df = train_df.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1)\n",
    "test_df = test_df.drop(['PassengerId','Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter tuning on decision tree\n",
    "\n",
    "# X_train = train_df.drop(\"Survived\", axis=1)\n",
    "# Y_train = train_df[\"Survived\"]\n",
    "# X_test  = test_df.copy()\n",
    "\n",
    "# decision_tree = DecisionTreeClassifier(criterion='gini')\n",
    "# decision_tree.fit(X_train, Y_train)\n",
    "# Y_pred = decision_tree.predict(X_test)\n",
    "# acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
    "# print('DT', acc_decision_tree)\n",
    "\n",
    "# print(decision_tree.get_params())\n",
    "\n",
    "# param_dict = {\n",
    "#     \"criterion\":['gini'],\n",
    "#     \"max_depth\":range(1,50),\n",
    "#     \"min_samples_split\":range(1,50),\n",
    "#     \"min_samples_leaf\":range(1,50)\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(decision_tree,\n",
    "#                    param_grid = param_dict,\n",
    "#                    cv=10,\n",
    "#                    verbose=1,\n",
    "#                    n_jobs=-1)\n",
    "\n",
    "# grid.fit(X_train, Y_train)\n",
    "\n",
    "# print(grid.best_params_)\n",
    "# print(grid.best_estimator_)\n",
    "# print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision Tree\n",
    "# X_train = train_df.drop(\"Survived\", axis=1)\n",
    "# Y_train = train_df[\"Survived\"]\n",
    "# X_test  = test_df.copy()\n",
    "\n",
    "# decision_tree = DecisionTreeClassifier(criterion='gini', max_depth = 15, min_samples_leaf = 1, min_samples_split = 2)\n",
    "# decision_tree.fit(X_train, Y_train)\n",
    "# Y_pred = decision_tree.predict(X_test)\n",
    "# acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
    "# print('Decision Tree Accuracy:', acc_decision_tree)\n",
    "\n",
    "# # print(decision_tree.get_params())\n",
    "\n",
    "# ### Drawing Decision Tree\n",
    "# feature_names = ['Pclass', 'Sex', 'Age', 'Fare', 'Family', 'Embarked', 'Deck', 'Alone']\n",
    "# class_names = ['0', '1']\n",
    "# dot_data = tree.export_graphviz(decision_tree, out_file=None,\n",
    "#                                 feature_names=feature_names,\n",
    "#                                 class_names=class_names,\n",
    "#                                 filled=True)\n",
    "# graph = graphviz.Source(dot_data, format=\"png\")\n",
    "# graph.render(\"Decision_tree_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5-fold cross-validation on the decision tree\n",
    "# scores = cross_val_score(decision_tree, X_train, Y_train, cv=5, scoring = \"accuracy\")\n",
    "# print(\"Scores:\", scores)\n",
    "# print(\"Mean accuracy after 5-fold cross-validation:\", scores.mean()*100)\n",
    "# print(\"Standard Deviation:\", scores.std()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random forest\n",
    "# random_forest = RandomForestClassifier(n_estimators=100)\n",
    "# random_forest.fit(X_train, Y_train)\n",
    "\n",
    "# Y_prediction = random_forest.predict(X_test)\n",
    "\n",
    "# random_forest.score(X_train, Y_train)\n",
    "# acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
    "# print('Random Forest Accuracy', acc_random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5-fold cross-validation on the random forest\n",
    "# scores = cross_val_score(random_forest, X_train, Y_train, cv=5, scoring = \"accuracy\")\n",
    "# print(\"Scores:\", scores)\n",
    "# print(\"Mean accuracy after 5-fold cross-validation:\", scores.mean()*100)\n",
    "# print(\"Standard Deviation:\", scores.std()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 68.69248634737303\n",
      "Mean Precision 67.90742484621836\n",
      "Mean Recall 34.51832907075874\n",
      "Mean f1_score 45.56274898483441\n"
     ]
    }
   ],
   "source": [
    "### SVM\n",
    "X_train = train_df.drop(\"Survived\", axis=1)\n",
    "Y_train = train_df[\"Survived\"]\n",
    "X_test  = test_df.copy()\n",
    "svc = svm.SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "\n",
    "scores = cross_validate(svc, X_train, Y_train, cv=5, scoring=scoring)\n",
    "\n",
    "print('Mean Accuracy:', scores['test_accuracy'].mean() * 100)\n",
    "print('Mean Precision', scores['test_precision'].mean() * 100)\n",
    "print('Mean Recall', scores['test_recall'].mean() * 100)\n",
    "print('Mean f1_score', scores['test_f1_score'].mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 79.1224656330425\n",
      "Mean Precision 74.93187061947673\n",
      "Mean Recall 68.41005967604433\n",
      "Mean f1_score 71.48742726340538\n"
     ]
    }
   ],
   "source": [
    "### SVM with linear kernel\n",
    "X_train = train_df.drop(\"Survived\", axis=1)\n",
    "Y_train = train_df[\"Survived\"]\n",
    "X_test  = test_df.copy()\n",
    "svc = svm.SVC(kernel = 'linear')\n",
    "svc.fit(X_train, Y_train)\n",
    "\n",
    "scores = cross_validate(svc, X_train, Y_train, cv=5, scoring=scoring)\n",
    "\n",
    "print('Mean Accuracy:', scores['test_accuracy'].mean() * 100)\n",
    "print('Mean Precision', scores['test_precision'].mean() * 100)\n",
    "print('Mean Recall', scores['test_recall'].mean() * 100)\n",
    "print('Mean f1_score', scores['test_f1_score'].mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 61.95342414161069\n",
      "Mean Precision 60.0\n",
      "Mean Recall 0.8780903665814151\n",
      "Mean f1_score 1.730848861283644\n"
     ]
    }
   ],
   "source": [
    "### SVM with quadratic kernel\n",
    "X_train = train_df.drop(\"Survived\", axis=1)\n",
    "Y_train = train_df[\"Survived\"]\n",
    "X_test  = test_df.copy()\n",
    "svc = svm.SVC(kernel='poly', degree=2)\n",
    "svc.fit(X_train, Y_train)\n",
    "\n",
    "scores = cross_validate(svc, X_train, Y_train, cv=5, scoring=scoring)\n",
    "\n",
    "print('Mean Accuracy:', scores['test_accuracy'].mean() * 100)\n",
    "print('Mean Precision', scores['test_precision'].mean() * 100)\n",
    "print('Mean Recall', scores['test_recall'].mean() * 100)\n",
    "print('Mean f1_score', scores['test_f1_score'].mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 68.69248634737303\n",
      "Mean Precision 67.90742484621836\n",
      "Mean Recall 34.51832907075874\n",
      "Mean f1_score 45.56274898483441\n"
     ]
    }
   ],
   "source": [
    "### SVM with RBF kernel\n",
    "X_train = train_df.drop(\"Survived\", axis=1)\n",
    "Y_train = train_df[\"Survived\"]\n",
    "X_test  = test_df.copy()\n",
    "svc = svm.SVC(kernel='rbf')\n",
    "svc.fit(X_train, Y_train)\n",
    "\n",
    "scores = cross_validate(svc, X_train, Y_train, cv=5, scoring=scoring)\n",
    "\n",
    "print('Mean Accuracy:', scores['test_accuracy'].mean() * 100)\n",
    "print('Mean Precision', scores['test_precision'].mean() * 100)\n",
    "print('Mean Recall', scores['test_recall'].mean() * 100)\n",
    "print('Mean f1_score', scores['test_f1_score'].mean() * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
